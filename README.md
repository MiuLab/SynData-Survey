# ğŸ˜ SynData-Survey
## ğŸ¦› Paper Architecture
### ğŸ§¸ Abstract
### ğŸ¦– Introduction
#### 1. Data Augmentation Concepts
> è§£é‡‹ data augmentation çš„ç¸½é«”æ¦‚å¿µå’Œæ–¹æ³•
> å¯ä»¥ç”¨å¹´ä»£å»è§£é‡‹

### ğŸ‘» Background
> è§£é‡‹ä¸‹é¢é€™äº›çš„æ¦‚å¿µ
1. RNN, LSTM
2. transformer
3. LLM
4. Prompt Engineering
5. GAN
6. CNN
7. Diffusion

### ğŸ APPROACHES
- åƒè€ƒ https://arxiv.org/abs/2107.13586 ï¼ˆç”¨æŠ€è¡“åˆ†ç”¨å¹´ä»£æ”¾é †åºï¼‰
#### 1. ğŸ’ Feature Engineering:
- TEXT: rule-based
- IMAGE: https://pytorch.org/vision/stable/transforms.html
- OTHERS
#### 2. ğŸ¦™ architecture engineering: 
- TEXT: RNN, LSTM, Transformer
- IMAGE: GAN
- OTHERS
#### 3. ğŸˆ pre-train and fine-tune:
- TEXT: Transformer,  LM(encoder, decoder, BERT,generation-based model, RNN)
- IMAGE: CNN, Diffusion
- OTHERS
#### 4. ğŸŸ pre-train, prompt, and predict:
- TEXT: Generation LLM(GPT), prompt engineering, context learning
- IMAGE: Diffusion, DALLE
- OTHERS

### ğŸ¦˜ AUGMENTATION OBJECTIVES
- improve diversity, ex: å°‘æ•¸è³‡æºçš„èªè¨€æ–°å¢
- improve dataset balance ex: label imbalance
- domain shift

### ğŸšŸ APPLICATION
- ğŸ“’ Text
  - Text classification
  - Question answering
  - Translation
  - Natural language inference
  - Text Generation
  - Summarizing
  - Instruction tuning
  - Others
- ğŸ–¼ï¸ Image
  - Image Classification 
  - Semantic Segmentation
  - Object Detection
  - Human Pose Estimation

### ğŸŠ Future Work
### ğŸ§‘ğŸ»â€ğŸ“ Conclusion

## ğŸ¤— Papers that we read
| é¡åˆ¥          | who are you | paper é¡Œç›®                                                                                                          | year | paper é€£çµ                                            | æŠ€è¡“é¡å‹ï¼ˆå››é¡ï¼‰                         | application                 | dataset  | ä¸€å¥è©±ç¸½çµ                                                                                                                                                                                                               | 
|-------------|-------------|-------------------------------------------------------------------------------------------------------------------|------|-----------------------------------------------------|----------------------------------|-----------------------------|----------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| text        | æ²›å¦¤          | Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks           | 2021 | https://arxiv.org/abs/2010.08240                    | pre-train and fine-tune          | Natural language inference  | quantity | ç”¨ cross-encoders æ¨™è¨˜è³‡æ–™ä¾†å¢å¼· Bi-encoders æ¨¡å‹                                                                                                                                                                             | æ²›å¦¤_10 papers               | æ²›å¦¤_10 papers               |                |
| text        | æ²›å¦¤          | AugGPT: Leveraging ChatGPT for Text Data Augmentation                                                             | 2023 | https://arxiv.org/abs/2302.13007                    | "pre-train, prompt, and predict" | Text classification         | quantity | é€é GPT æ“´å……èˆŠè³‡æ–™ + æ–°è³‡æ–™ => å¸Œæœ›åœ¨æ–°è³‡æ–™ä¸Šè¡¨ç¾å¥½                                                                                                                                                                                    |                            |                            |                |
| text        | æ²›å¦¤          | EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks                      | 2019 | https://arxiv.org/abs/1901.11196                    | feature engineering              | Text classification         | quantity | å°±æ˜¯å¾ˆç›´è¦ºçš„å››ç¨®èª¿æ›æ–‡å­—çš„æ–¹å¼ä¾† augï¼Œå¯ä»¥ç•¶ baseline                                                                                                                                                                                   |                            |                            |                |
| text        | æ²›å¦¤          | AEDA: An Easier Data Augmentation Technique for Text Classification                                               | 2021 | https://arxiv.org/abs/2108.13230                    | feature engineering              | Text classification         | quantity | åœ¨åŸå§‹æ–‡æœ¬ä¸­éš¨æ©Ÿæ’å…¥æ¨™é»ç¬¦è™Ÿï¼Œæ¯” EDA å¥½                                                                                                                                                                                              |                            |                            |                |
| text        | æ²›å¦¤          | Text Smoothing: Enhance Various Data Augmentation Methods on Text Classification Tasks                            | 2022 | https://arxiv.org/pdf/2202.13840.pdf                | architecture engineering         | Text classification         | quantity | text smoothing å­¸ç¿’ token çš„æ©Ÿç‡åˆ†ä½ˆï¼Œè€Œä¸åƒ…æ˜¯å®ƒæ˜¯å¦å‡ºç¾                                                                                                                                                                             |                            |                            |                |
| text        | æ²›å¦¤          | AUGNLG: Few-shot Natural Language Generation using Self-trained Data Augmentation                                 | 2021 | https://arxiv.org/pdf/2106.05589.pdf                | architecture engineering         | Text Generation             | quantity | æª¢ç´¢é—œéµå­—åŒ¹é… -> éæ¿¾é ˜åŸŸç„¡é—œ -> åˆæˆæ¨™è¨˜æ•¸æ“š                                                                                                                                                                                         |                            |                            |                |
| text        | æ²›å¦¤          | End-to-End Synthetic Data Generation for Domain Adaptation of Question Answering Systems                          | 2020 | https://arxiv.org/pdf/2010.06028.pdf                | architecture engineering         | Question answering          | quantity | å°‡æ®µè½è¼¸å…¥ encoder ä¸­ï¼Œdecoder é€è©ç”Ÿæˆå•é¡Œå’Œç­”æ¡ˆï¼Œä¸¦ä½¿ç”¨ç”Ÿæˆä¸­çš„å€¼ä½œç‚ºéæ¿¾åˆ†æ•¸ç¯©é¸ç”Ÿæˆæ•¸æ“š                                                                                                                                                              |                            |                            |                |
| text        | æ²›å¦¤          | GDA: Generative Data Augmentation Techniques for Relation Extraction Tasks                                        | 2023 | https://arxiv.org/pdf/2305.16663.pdf                | pre-train and fine-tune          | Text Generation             | quality  | one encoder é æ¸¬åŸå§‹å¥ã€one encoder ç”Ÿæˆç›¸ä¼¼å¥æ³•çš„ç›®æ¨™å¥                                                                                                                                                                            |                            |                            |                |
| text        | æ²›å¦¤          | Do Not Have Enough Data? Deep Learning to the Rescue!                                                             | 2019 | https://arxiv.org/pdf/1911.03118.pdf                | "pre-train, prompt, and predict" | Text classification         | quantity | åˆ©ç”¨ GPT-2 ä¾†é€²è¡Œ data augï¼Œå¾è€Œåœ¨ few-shot å ´æ™¯ä¸‹é”åˆ°å¾ˆå¥½çš„å¢å¼·æ•ˆæœ                                                                                                                                                                     |                            |                            |                |
| text        | æ²›å¦¤          | Training Question Answering Models From Synthetic Data                                                            | 2020 | https://arxiv.org/abs/2002.09599                    | "pre-train, prompt, and predict" | Question answering          | quantity | ç­”æ¡ˆç”Ÿæˆ -> å•é¡Œç”Ÿæˆ -> roundtrip filtration                                                                                                                                                                                |                            |                            |                |
| text        | QQ          | Adapting Neural Machine Translation with Parallel Synthetic Data                                                  | 2017 | https://aclanthology.org/W17-4714/                  | feature engineering              | Translation                 | quantity | é€écosine similarity è¼”åŠ©è¨ˆç®—ï¼Œæ‰¾åˆ°å’Œtarget domainç›¸é—œçš„å¥å­é€²è¡Œè³‡æ–™çš„æ“´å……ä¸¦é€²è¡Œç¿»è­¯ä¾†æ“´å…… data                                                                                                                                                  | å¿ƒç‘œ_æ–‡ä»¶                      | å¿ƒç‘œ_ç°¡å ±                      | 10 ç¯‡çœŸçš„æ˜¯å¤ªå¤šäº†å§ï¼ï¼ï¼ |
| text        | QQ          | MulDA: A Multilingual Data Augmentation Framework for Low-Resource Cross-Lingual NER                              | 2021 | https://aclanthology.org/2021.acl-long.453/         | pre-train and fine-tune          | Translation                 | quantity | é¦–å…ˆé€šéè¨“ç·´ LSTM-LMä¾†ç”¢ç”Ÿç‰¹å®šèªè¨€çš„synthetic dataï¼Œå†é€é mBART æ­é… mask ç”¢ç”Ÿä¸åŒ diversity çš„synthetic data                                                                                                                               |                            |                            |                |
| text        | QQ          | XLA: A Robust Unsupervised Data Augmentation Framework for Cross-Lingual NLP                                      | 2021 | https://openreview.net/forum?id=w5uur-ZwCXn         | pre-train and fine-tune          | Translation                 | quantity | XLAæ˜¯ä¸€å€‹ robust unsupervised cross-lingual augmentation framework é‡å° cross-lingual generalizationçš„LMä¾†å¢åŠ  data                                                                                                          |                            |                            |                |
| text        | QQ          | Data Augmentation for Abstractive Query-Focused Multi-Document Summarization                                      | 2021 | https://arxiv.org/abs/2103.01863                    | feature engineering              | Summarizing                 | quantity | å»ºç«‹äº†2å€‹  query - summary dataset ï¼Œä¸€å€‹æ˜¯æœ‰çœŸå¯¦çš„ summary å»ºç«‹è™›æ“¬çš„ queryï¼Œä¸€å€‹æ˜¯æœ‰çœŸå¯¦çš„ query å»ºç«‹è™›æ“¬çš„ summary                                                                                                                             |                            |                            |                |
| text        | QQ          | Paraphrase Augmented Task-Oriented Dialog Generation                                                              | 2020 | https://aclanthology.org/2020.acl-main.60/          | pre-train and fine-tune          | Dialogue                    | quantity | é¦–å…ˆåšParaphrase Data augmentationç„¶å¾Œå†ç”¨encoder-decoder ç”ŸæˆResponse                                                                                                                                                       |                            |                            |                |
| text        | QQ          | Transforming Wikipedia into Augmented Data for Query-Focused Summarization                                        | 2022 | https://arxiv.org/abs/1911.03324                    | feature engineering              | Summarizing                 | quantity | é€éWIKIPEDIA å»ºç«‹ä¸€å€‹ query-summarization çš„ Dataset                                                                                                                                                                      |                            |                            |                |
| text        | QQ          | Data Augmentation using Pre-trained Transformer Models                                                            | 2021 | https://arxiv.org/abs/2003.02245                    | pre-train and fine-tune          | Text classification         | quantity | ä½¿ç”¨ä¸‰ç¨®ä¸åŒçš„LMä¾†åšData augmentationåœ¨ä¸‰ç¨®ä¸åŒçš„taskä¸Šï¼Œçœ‹çœ‹æ•ˆæœå¦‚ä½•                                                                                                                                                                      |                            |                            |                |
| text        | QQ          | Mitigating Class Imbalance in Sentiment Analysis through GPT-3-Generated Synthetic Sentences                      | 2023 | https://www.mdpi.com/2076-3417/13/17/9766           | pre-train and fine-tune          | Text classification         | quality  | ç•¶åˆ†é¡å•é¡Œçš„è¨“ç·´è³‡æ–™é›†ä¸­å‡ºç¾æ•¸é‡ä¸å¹³è¡¡çš„å•é¡Œæ™‚ï¼Œä½¿ç”¨GPT-3åšData augmentationä¾†å°‡ Dataset è®Šæˆå¹³è¡¡çš„ Data                                                                                                                                              |                            |                            |                |
| text        | QQ          | DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM Workflows                                  | 2024 | https://arxiv.org/abs/2402.10379                    | "pre-train, prompt, and predict" | Text Generation             | quantity | ä¸€å€‹æ–¹ä¾¿ä½¿ç”¨LMçš„å·¥å…·ï¼Œå¦‚æœæˆ‘å€‘è¦åšå¯¦é©—çš„è©±å¯ä»¥è€ƒæ…®è©¦è©¦çœ‹é€™å€‹å·¥å…·                                                                                                                                                                                   |                            |                            |                |
| text        | QQ          | Synthetic Dialogue Dataset Generation using LLM Agents                                                            | 2024 | https://arxiv.org/abs/2401.17461                    | "pre-train, prompt, and predict" | Dialogue                    | quantity | Dual LLM ä½¿ç”¨ prompt engineering ä¾†æ¨¡æ“¬å°è©±ï¼Œç”¢ç”Ÿ dialog data                                                                                                                                                                 |                            |                            | çµ‚æ–¼çœ‹å®Œäº†ï¼ï¼ï¼       |
| text        | Victoria    | WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation                                | 2022 | https://arxiv.org/pdf/2201.05955.pdf                | "pre-train, prompt, and predict" | Natural language inference  | quantity | ä»¥gpt3ä½¿ç”¨MultiNLI dateset ç”¢ç”Ÿç›¸å°æˆ–é¡ä¼¼çš„å¹¾å€‹å¥å­ï¼Œéæ¿¾å¾Œç”±äººé¡evaluate                                                                                                                                                                 | Victoria                   | Victoria_ç°¡å ±                |                |
| text        | Victoria    | Data Augmentation for Intent Classification with Off-the-shelf Large Language Models                              | 2022 | https://arxiv.org/pdf/2204.01959.pdf                | "pre-train, prompt, and predict" | Text classification         | quality  | ä½¿ç”¨GPT3ä»¥few shotæ–¹å¼ç”¢ç”Ÿintent classification data                                                                                                                                                                       |                            |                            |                |
| text        | Victoria    | GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation                                             | 2021 | https://arxiv.org/pdf/2104.08826.pdf                | "pre-train, prompt, and predict" | Text classification         | quantity | åˆ©ç”¨GPT3å³pseudo labelingçš„æ–¹å¼æ“´å¢è³‡æ–™                                                                                                                                                                                       |                            |                            |                |
| text        | Victoria    | Character-level Convolutional Networks for Text Classification                                                    | 2016 | https://arxiv.org/pdf/1509.01626.pdf                | feature engineering              | Text classification         | quantity | é¸æ“‡åˆ©ç”¨å®ƒå€‘çš„åŒç¾©è©æ›¿æ›å–®è©æˆ–çŸ­èªï¼Œlibraryä¾†è‡ªLibreOffice                                                                                                                                                                              |                            |                            |                |
| text        | Victoria    | Generative Data Augmentation for Commonsense Reasoning                                                            | 2020 | https://aclanthology.org/2020.findings-emnlp.90.pdf | "pre-train, prompt, and predict" | Question answering          | quality  | åˆ†åˆ¥fine tune å…©å€‹LMï¼Œä¸€å€‹æ˜¯Generator(GPT2)ï¼Œä¸€å€‹æ˜¯organic training(RoBERTa)ï¼Œå°generatorç”¢ç”Ÿçš„dataåšrelabelingï¼Œå†åšinfluenceå’Œdiversity filtering                                                                                       |                            |                            |                |
| text        | Victoria    | Data Augmentation for Low-Resource Neural Machine Translation                                                     | 2017 | https://arxiv.org/pdf/1705.00440.pdf                | architecture engineering         | Translation                 | quality  | ç¿»è­¯è³‡æ–™æ“´å¢ï¼ˆTDAï¼‰ï¼Œå®ƒé€šéæ”¹è®Šå¹³è¡Œèªæ–™ä¸­ç¾æœ‰çš„å¥å­ä¾†æ“´å¢è¨“ç·´è³‡æ–™ï¼Œèˆ‡é›»è…¦è¦–è¦ºä¸­çš„è³‡æ–™æ“´å¢æ–¹æ³•ç›¸ä¼¼                                                                                                                                                                  |                            |                            |                |
| text        | Victoria    | Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations                                   | 2018 | https://arxiv.org/pdf/1805.06201.pdf                | architecture engineering         |                             | quantity | æå‡ºäº†ä¸Šä¸‹æ–‡å¢å¼·ï¼Œç”¨æ–¼å¢å¼·å–®è©èˆ‡æ›´å¤šä¸åŒçš„å–®è©ã€‚ä¸ä½¿ç”¨åŒç¾©è©ï¼Œè€Œæ˜¯ä½¿ç”¨æ ¹æ“šåŸå§‹å–®è©æ‰€è™•ä¸Šä¸‹æ–‡é æ¸¬å‡ºçš„å–®è©é€²è¡Œå¢å¼·ã€‚                                                                                                                                                           |                            |                            |                |
| text        | Victoria    | Few-shot learning through contextual data augmentation                                                            | 2021 | https://aclanthology.org/2021.eacl-main.90.pdf      | "pre-train, prompt, and predict" | Translation                 | quantity | æå‡ºContextual data augmentationçš„æ–¹æ³•                                                                                                                                                                                   |                            |                            |                |
| text        | Victoria    | Improving Few-shot Generalization of Safety Classifiers via Data Augmented Parameter-Efficient Fine-Tuning        | 2023 | https://arxiv.org/pdf/2310.16959.pdf                | "pre-train, prompt, and predict" | Text classification         | quality  | æå‡ºDAPTï¼Œæ¯”è¼ƒ prompt tuning(PEFT) å’Œ ICL (in-context learning) ï¼Œèªç‚ºPEFTè¼ƒå¥½                                                                                                                                                 |                            |                            |                |
| text        | Victoria    | FlipDA: Effective and Robust Data Augmentation for Few-Shot Learning                                              | 2022 | https://aclanthology.org/2022.acl-long.592.pdf      | "pre-train, prompt, and predict" | Text classification         | quality  | åˆ©ç”¨T5ç”¢ç”Ÿæ­£åå…©é¢çš„å€™é¸äººï¼Œè¼¸å…¥é€²classifierï¼Œè‹¥è¼¸å‡ºlabelä¸ä¸€è‡´å‰‡åˆªæ‰                                                                                                                                                                         |                            |                            |                |
| image       | å‘¨æ•¦ç¿”         | CamDiff: Camouflage Image Augmentation via Diffusion Model                                                        | 2023 | https://arxiv.org/abs/2304.05469                    | "pre-train, prompt, and predict" | object detection            | quality  | Augment existing camouflage object detection (COD) dataset with salient objects to enhance the robustness of COD models.                                                                                            | Image Augmentation æ•¦ç¿”      | Image Augmentation æ•¦ç¿”      |                |
| image       | å‘¨æ•¦ç¿”         | Generating images of rare concepts using pre-trained diffusion models                                             | 2023 | https://arxiv.org/abs/2304.14530                    | pre-train and fine-tune          | image classification        | quantity | Rare concepts can be correctly generated by carefully selecting suitable generation seeds in the noise space                                                                                                        |                            |                            |                |
| image       | å‘¨æ•¦ç¿”         | Diversify Your Vision Datasets with Automatic Diffusion-Based Augmentation                                        | 2023 | https://arxiv.org/abs/2305.16289                    | "pre-train, prompt, and predict" | image classification        | quantity | Augment the training data via descriptions of the domains seen in training data and language-guided image editing                                                                                                   |                            |                            |                |
| image       | å‘¨æ•¦ç¿”         | Semantic Generative Augmentations for Few-Shot Counting                                                           | 2023 | https://arxiv.org/abs/2311.16122                    | pre-train and fine-tune          | object detection            | quantity | "Synthesize few-shot object counting data with stable diffusion, conditioned on a textual prompt and a density map"                                                                                                 |                            |                            |                |
| image       | å‘¨æ•¦ç¿”         | Diffusion-based Data Augmentation for Nuclei Image Segmentation                                                   | 2024 | https://arxiv.org/abs/2310.14197                    | architecture engineering         | semantic segmentation       | quantity | Introduce a two-step strategy for diffusion-based nuclei segmentation augmentation                                                                                                                                  |                            |                            |                |
| image       | å‘¨æ•¦ç¿”         | GPT-Prompt Controlled Diffusion for Weakly-Supervised Semantic Segmentation                                       | 2023 | https://arxiv.org/abs/2310.09760                    | "pre-train, prompt, and predict" | semantic segmentation       | quantity | "Utilizes GPT with image labels to generate diverse prompts, and use the diffusion model to synthesize images. "                                                                                                    |                            |                            |                |
| image       | å‘¨æ•¦ç¿”         | Towards Generalizable Tumor Synthesis                                                                             | 2024 | https://arxiv.org/abs/2402.19470                    | pre-train and fine-tune          | semantic segmentation       | quantity | Diffusion Models can create realistic tumors generalized to a range of organs even when trained on tumor examples from only one organ.                                                                              |                            |                            |                |
| multi-modal | å‘¨æ•¦ç¿”         | VIXEN: Visual Text Comparison Network for Image Difference Captioning                                             | 2024 | https://arxiv.org/abs/2402.19119                    | "pre-train, prompt, and predict" | image difference captioning | quantity | Training on synthetically manipulated images to improve image difference captioning                                                                                                                                 |                            |                            |                |
| image       | å‘¨æ•¦ç¿”         | ShapeBoost: Boosting Human Shape Estimation with Part-Based Parameterization and Clothing-Preserving Augmentation | 2024 | https://arxiv.org/abs/2403.01345                    | feature engineering              | human pose estimation       | quantity | Propose a clothing-preserving data augmentation module to generate realistic images with diverse body shapes for human shape recovery.                                                                              |                            |                            |                |
| image       | å‘¨æ•¦ç¿”         | Boosting Dermatoscopic Lesion Segmentation via Diffusion Models with Visual and Textual Prompts                   | 2023 | https://arxiv.org/abs/2310.02906                    | pre-train and fine-tune          | semantic segmentation       | quantity | Adapt diffusion model with lesion-specific visual and textual prompts for generating skin lesion images.                                                                                                            |                            |                            |                |
| image       | é«˜é•·è–         | Diversity is Definitely Needed: Improving Model-Agnostic Zero-shot Classification via Stable Diffusion            | 2023 | https://arxiv.org/pdf/2302.03298.pdf                | "pre-train, prompt, and predict" | image classification        | quality  | Enhance the diversity of generated images through different prompts to increase the performance of the image classification task using the diffusion model.                                                         | Synthetic Data Paper Intro | Synthetic Data Paper Intro |                |
| image       | é«˜é•·è–         | DALL-E for Detection: Language-driven Compositional Image Synthesis for Object Detection                          | 2022 | https://arxiv.org/abs/2206.09592                    | "pre-train, prompt, and predict" | object detection            | quantity | "Allow DALL-E to generate background and foreground objects separately, then combine them, so that the synthetic data for tasks such as object detection or semantic segmentation are generated"                    |                            |                            |                |
| image       | é«˜é•·è–         | DiffuMask: Synthesizing Images with Pixel-level Annotations for Semantic Segmentation Using Diffusion Models      | 2023 | https://arxiv.org/abs/2303.11681                    | "pre-train, prompt, and predict" | semantic segmentation       | quantity | "While generating images using the diffusion model, synthetic semantic segmentation data is simultaneously generated through cross-attention."                                                                      |                            |                            |                |
| image       | é«˜é•·è–         | Dataset Diffusion: Diffusion-based Synthetic Dataset Generation for Pixel-Level Semantic Segmentation             | 2023 | https://arxiv.org/abs/2309.14303                    | "pre-train, prompt, and predict" | semantic segmentation       | quantity | Generate synthetic semantic segmentation data containing multiple different classes and utilize self-attention to refine cross-attention maps.                                                                      |                            |                            |                |
| image       | é«˜é•·è–         | DatasetDM: Synthesizing Data with Perception Annotations Using Diffusion Models                                   | 2023 | https://arxiv.org/abs/2308.06160                    | "pre-train, prompt, and predict" | semantic segmentation       | quantity | "By decoding the latent code of the diffusion model, label different tasks, thereby generating synthetic data for different tasks"                                                                                  |                            |                            |                |
| text        | é«˜é•·è–         | Self-Instruct: Aligning Language Models with Self-Generated Instructions                                          | 2022 | https://arxiv.org/abs/2212.10560                    | "pre-train, prompt, and predict" | Instruction tuning          | quantity | Develop a framework to leverage the in-context learning capabilities of Large Language Models (LLM) for generating synthetic text data.                                                                             |                            |                            |                |
| multi-modal | é«˜é•·è–         | Visual Instruction Tuning                                                                                         | 2023 | https://arxiv.org/abs/2304.08485                    | "pre-train, prompt, and predict" | Instruction tuning          | quantity | Generate a visual instruction tuning dataset through self-instruction.                                                                                                                                              |                            |                            |                |
| multi-modal | é«˜é•·è–         | MIMIC-IT: Multi-Modal In-Context Instruction Tuning                                                               | 2023 | https://arxiv.org/abs/2306.05425                    | "pre-train, prompt, and predict" | Instruction tuning          | quantity | "Generate synthetic visual instruction datasets for various domains (image, video, etc.) through the self-instruct method."                                                                                         |                            |                            |                |
| multi-modal | é«˜é•·è–         | Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning                                | 2023 | https://arxiv.org/abs/2306.14565                    | "pre-train, prompt, and predict" | Instruction tuning          | quantity | "Enhance the method of visual instruction tuning by incorporating information such as dense captions, object detection, and other details, treating them as image representations in the self-instruction process." |                            |                            |                |
| multi-modal | é«˜é•·è–         | DialogCC: Large-Scale Multi-Modal Dialogue Dataset                                                                | 2022 | https://arxiv.org/abs/2212.04119                    | "pre-train, prompt, and predict" | Dialogue                    | quantity | Create a synthetic visual dialogue dataset by leveraging a text-only dialogue dataset and an image captioning dataset                                                                                               |                            |                            |                |
